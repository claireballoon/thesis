\section{User Requirements}

The objective of this program is to classify tweets using the \#MeToo hashtag, and the core is to output a categorization that is as accurate as possible. The program will attach three levels of classification to each tweet. In detail, the classification requirements are:

1)  The first check is asking whether or not the tweet is relevant to the \#MeToo movement and awarded either a 1 or 2 (tweets in languages other than English are eliminated from the dataset).

2) The second check determines whether or not the tweet is in support of the movement, against the movement, or questioning the movement and awarded a 1, 2, or 3 accordingly. If there is not enough information to make this determination, this variable is left blank. If the tweet is expressing an experience, at this stage it should be awarded a 1.

3) The third check attempts to categorize the specific type of harassment. If there is not enough context, or if the tweet is merely \#MeToo without other context, it is categorized with a D. If the tweet is expressing an experience with sexist or patronizing behavior, it is marked A. If the tweet is expressing an experience with unwanted sexual attention, it is marked B. If the tweet is expressing an experience with predatory behavior, it is marked C.

There are more factors considered when categorizing a tweet. Only text within the tweet will be reviewed with the use of this program; no images, or links to text on other sites will be included. Tweets that do not contain enough context to properly categorize them will have the categorizations left blank accordingly and stop at whatever check they reached. Words contained within other hashtags will be included when evaluating context, but the words contained within username mentions will not be considered. Experiences that occurred when the victim was a minor will be categorized as predatory, and ambiguous descriptions using the \#MeToo hashtag are categorized as not having enough context. Some samples are provided below:

*will be a figure one day*

\section{Development, Design, and Deployment}

This project is written with the Python programming language, using the NLTK, spaCy, and and pandas libraries for language processing and the scikit-learn library for classification algorithms. Jupityr will be used to handle the preprocessing and exporting of the dataset. Matplotlib has been used be used for basic visualizations using the completed dataset. Python was chosen for its flexibility from the high quantity of libraries available to assist in natural language processing and for supervised machine learning.


Dr. Hassan has already collected the dataset, which amasses to approximately half a million tweets of data. Currently, I have manually labeled 1,000 of the tweets according to the proper categorization with the intention to raise that number up to 10,000 manually labeled tweets for the purposes of supervised machine learning. To verify the integrity of my categorization definitions, multiple volunteers of both genders are categorizing the dataset alongside me. The project is hosted on my personal GitHub account under the username “claireballoon”.

While the program can categorize a local dataset simply by running it in Python3 and outputting a file with proper categorization, further utility is desired from this project. The immediate step after completion of the categorization is to create a simple online interface for researchers to use to categorize their data without having to install Python and categorize them locally. Accomplish our core goals necessitates only the installation of Python3 and the appropriate libraries for those wishing to categorize their own datasets of tweets. As Jupityr can export to PDF or LaTeX, no further environment specifications are needed for researchers looking to use the results of this categorization in their data at this time.

\section{Architecture}

The overall architecture is very simple. It requires the raw data, the properly formatted dataset, the natural language processing and machine-learning algorithms, the formatted categorization results, and the visualizations that can be made from those results. The process flows linearly.

//todo - figure

\section{User Interfaces}

At this time, the implementation of the project only requires use of Python, the command line, and a text file with the dataset of tweets formatted properly (ID of the tweet and the text). The program will then output a new file with the categorizations and possibly some visualizations. In addition to creating visualizations with the user’s data, the master dataset would have utility in being posted publicly so that others researching sexual harassment can use the categorized data in their work. The easy, next step is to create an online interface that doesn’t require the user to categorize their dataset locally.

\section{Testing and Integration}

The manual labeling of tweets will be used to verify whether or not the program has successfully categorized the tweets. By manually categorizing a large enough dataset (goal set for 10,000), it can be determined with a high degree of accuracy how successful the algorithm and project will be. 
